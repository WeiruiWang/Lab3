{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Running Pyspark inÂ Colab**\n",
        "\n",
        "To run spark in Colab, You need proper setup. Next cell will create required setup for running Spark/PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "j0VTrimTI_9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/BostonHousing.csv\"\n",
        "\n",
        "# (1)Read the CSV file\n",
        "df = spark.read.csv(path, header = True, inferSchema = True)\n",
        "\n",
        "\n",
        "# (1) Show the first 5 elements of the dataset\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "AjZx-Z-bGOK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) Count number of observations in the dataset\n",
        "observation_count = df.count()\n",
        "print(f\"Number of observations: {observation_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73pfPk7PKB-k",
        "outputId": "6ccccd87-e47c-4cae-ecd2-869e2633499d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observations: 506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4)Show the Schema of  dataset\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC7kz_AUKHDt",
        "outputId": "b3653b9c-57e0-4be6-8895-e29712b805b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- crim: double (nullable = true)\n",
            " |-- zn: double (nullable = true)\n",
            " |-- indus: double (nullable = true)\n",
            " |-- chas: integer (nullable = true)\n",
            " |-- nox: double (nullable = true)\n",
            " |-- rm: double (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- dis: double (nullable = true)\n",
            " |-- rad: integer (nullable = true)\n",
            " |-- tax: integer (nullable = true)\n",
            " |-- ptratio: double (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- lstat: double (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5)Drop the column \"b\" from dataset\n",
        "df = df.drop(\"b\")\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6xW0wifKLhz",
        "outputId": "662a3f61-b6cc-4f71-9574-c1e36dfc0d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- crim: double (nullable = true)\n",
            " |-- zn: double (nullable = true)\n",
            " |-- indus: double (nullable = true)\n",
            " |-- chas: integer (nullable = true)\n",
            " |-- nox: double (nullable = true)\n",
            " |-- rm: double (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- dis: double (nullable = true)\n",
            " |-- rad: integer (nullable = true)\n",
            " |-- tax: integer (nullable = true)\n",
            " |-- ptratio: double (nullable = true)\n",
            " |-- lstat: double (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# (6)Round all of the numerical columns into two decimal places\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "numerical_columns = [field.name for field in df.schema.fields if field.dataType in [\"IntegerType\", \"LongType\", \"DoubleType\", \"FloatType\"]]\n",
        "\n",
        "for column in numerical_columns:\n",
        "    df = df.withColumn(column, round(df[column], 2))\n"
      ],
      "metadata": {
        "id": "YCKjN-gRKQk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (7)Create a new column (Age10) with 10% increasing of 'age' column\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"Age10\", col(\"age\") * 1.10)\n",
        "\n",
        "# (8)Plot histogram Age10 column on a 2D Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert Spark DataFrame to Pandas for plotting\n",
        "age10_pd = df.select(\"Age10\").toPandas()\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(age10_pd[\"Age10\"], bins=30, edgecolor='black')\n",
        "plt.xlabel('Age10')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Age10')\n",
        "plt.show()\n",
        "\n",
        "# (9)Provide Summary Statistics of all columns (count, mean, stddev, min, max)\n",
        "summary_statistics = df.describe()\n",
        "summary_statistics.show()\n",
        "\n",
        "# (10)Convert Spark DataFrame to Pandas DataFrame\n",
        "pandas_df = df.toPandas()\n",
        "\n",
        "# Show the last 5 rows of Pandas DataFrame\n",
        "print(pandas_df.tail())"
      ],
      "metadata": {
        "id": "4_ic1oC3PliG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}